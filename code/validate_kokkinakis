import yaml
import pickle
from pathlib import Path
from src import preprocessing, classification, vizualisation, utils, spectral_analysis
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from pqdm.processes import pqdm
import numpy as np
from pqdm.processes import pqdm
import json

# Declare file locations
# root_dir = Path.home() / "Git" / "minimap_SSVEPs"
root_dir = Path('G:\workshop\minimap_SSVEPs')
data_dir = root_dir / "data"
logs_dir= root_dir / "results" / "kokkinakis"
fig_dir = root_dir / "results" / "kokkinakis" / "figures"
params_dir = root_dir / "code" / "conf"

# Load params
with open(params_dir / "parameters.yaml", 'r') as file:
    parameters = yaml.safe_load(file)
# Initialize vizualisation settings
vizualisation.sns_styleset()

# Get paths for data files
left_data_folder = preprocessing.get_all_sample_filenames(
    data_dir / "kokkinakis" / "left_minimap_eeg_data",
    "edf"
)
right_data_folder = preprocessing.get_all_sample_filenames(
    data_dir / "kokkinakis" / "right_minimap_eeg_data",
    "edf"
)

# Load data if already preprocessed
try:
    left_minimap_epochs, right_minimap_epochs = utils.load_epochs(data_dir / "kokkinakis" / "epochs.pickle") 
# Otherwise preprocess it
except TypeError:
    left_minimap_epochs = preprocessing.ingest_samples(
            left_data_folder,
            parameters["preprocessing"]
    )
    right_minimap_epochs = preprocessing.ingest_samples(
            right_data_folder,
            parameters["preprocessing"]
    )
    # Equalize the number of epochs for each event between the two sets
    preprocessing.equalize_epoch_counts(
            left_minimap_epochs,
            right_minimap_epochs
    )
    # Pickle the data for later
    with open(data_dir / "kokkinakis" / "epochs.pickle", 'wb') as f:
        pickle.dump([left_minimap_epochs, right_minimap_epochs], f
        )


# Set labels for plots
group_labels = {
    left_minimap_epochs: 'left_group',
    right_minimap_epochs: 'right_group'
}


# ## here we plot time courses of the epochs across conditions
# for group in [left_minimap_epochs, right_minimap_epochs]:
#     for freq in ['3.75', '4.8']:
#         vizualisation.plot_time_course(
#             epochs = group[f'{freq}_LEFT', f'{freq}_RIGHT'],
#             ROIs = parameters['analysis']['vROIs'],
#             group_labels = group_labels,
#             fig_name = f'{group_labels[group]}_{freq}_average_time_courses_visual_channels',
#             fig_dir = fig_dir / "time_courses"
#         )

# ## Here we classify left- versus right- conditioned participants based on their EEG data
# # Cross-validate classifier and return metrics
# log_res_metrics = classification.main_analysis(
#                     left_minimap_epochs,
#                     right_minimap_epochs,
#                     LogisticRegression,
#                     logs_dir,
#                     random_state=parameters["preprocessing"]["random_state"],
#                     permutation_test = False,
#                     max_iter=200
# )
# rf_metrics = classification.main_analysis(
#                     left_minimap_epochs,
#                     right_minimap_epochs,
#                     RandomForestClassifier,
#                     logs_dir,
#                     random_state=parameters["preprocessing"]["random_state"],
#                     permutation_test = False
# )
# svm_metrics = classification.main_analysis(
#                     left_minimap_epochs,
#                     right_minimap_epochs,
#                     SVC,
#                     logs_dir,
#                     random_state=parameters["preprocessing"]["random_state"],
#                     permutation_test = False,
#                     kernel="linear"
# )


# These constants have to be somewhat hardcoded due to the need for multiproc functions to be top-level
MODEL_CLASS = RandomForestClassifier
K_FOLDS = 5
RANDOM_STATE = 42

def parallel_lofo(
    channel_name
):
    """
    parallel_lofo is a function designed to calculate leave-one-feature-out importance 
    in a parallelised fashion.

    The implementation here is very simple and will produce an output json 
    that requires further parsing. 

    Note that parallelising this function is very memory heavy, and failure to adequately
    manage memory will result in numpy array memory errors, which are not currently handled.
    
    Usage:
        Because of the way our current analysis pipeline is set up, this is written with 
        a requirement for hardcoding the model parameters. Sorry. To this end, see the constants
        I defined above. You will similarly need to supply constants for any extra args like LOGREG_NUM_ITER. 
        Once that's done, simply use pqdm as written here (or process pools if you prefer) to compute each
        feature importance in parallel. 


    Args:
        channel_name (list): this is a list of all the channel names (features) that we want to validate (calculate importance) for. 

    Returns:
        dict: This function technically returns a single dict of dicts, which contains the usual crossval metrics. 
            In the usage below, observe that these dicts are stored in a list, resulting from the multiprocessing.
    """
    first_epochs_copy = left_minimap_epochs.copy()
    second_epochs_copy = right_minimap_epochs.copy()
    
    first_epochs_copy.drop_channels(channel_name, "warn")
    second_epochs_copy.drop_channels(channel_name, "warn")

    X_first = classification.compute_psd_and_features(first_epochs_copy) 
    X_second = classification.compute_psd_and_features(second_epochs_copy)
    X = np.vstack([X_first, X_second])
    y = classification.create_labels_for_binary_classification(
        len(first_epochs_copy.events),
        len(second_epochs_copy.events),
    )
    # Perform cross-validation on the training set
    cv_results = classification.perform_cross_validation(X, y, MODEL_CLASS, K_FOLDS, RANDOM_STATE)
    cv_metrics = classification.parse_cv_results(cv_results, K_FOLDS)
    out = {channel_name: cv_metrics}
    return out

if __name__ == "__main__":

    # I think this is a necessary and worthwhile check before running LOFO, since we have two datasets to align.
    assert left_minimap_epochs.ch_names == right_minimap_epochs.ch_names
    channel_names = left_minimap_epochs.ch_names

    # I have chosen to use pqdm to attach a progress bar to this, but using joblib, etc. would be fine too.
    results = pqdm(channel_names, parallel_lofo, n_jobs=6)

    with open(logs_dir/"rf_validation.json", "wb") as results_file:
        json.dump(results, results_file, indent=4)
